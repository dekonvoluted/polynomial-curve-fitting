
\documentclass{article}

\usepackage[utf8]{inputenc} % Encoding on input fonts set to UTF-8
\usepackage[T1]{fontenc} % Font encoding set to T1 fonts
\usepackage{fullpage} % Margins reduced to 1 in
\usepackage{mathptmx} % Use Times font
\usepackage[version=3]{mhchem} % Use chemical formulae
\usepackage[colorlinks]{hyperref} % Use links

\title{Polynomial Curve Fitting}
\author{Karthik Periagaram}
\date{\today}

\begin{document}

\maketitle

\section{A Note on Notation}

In what follows, scalars, (column-)vectors and matrices are represented consistently.
A scalar, such as \(n\) (the number of observations) is written in lowercase.
A vector, such as \(\vec{X}\) (the input vector) is always a column vector and is written in uppercase and has an arrow over it.
Finally, a matrix is written in uppercase, example, \(A\vec{X}=\vec{B}\).

\section{Problem Statement}

We begin with \(n\) observations acquired as distinct values of an input variable, \(x\).
The values of the input variable at which these observations are made, \(x_i\) form the components of the input vector, \(\vec{X}\).
The values are distinct and arranged in a monotonically ascending order, such that \(x_1\) is the lowest and \(x_n\) is the highest.
This range, \([x_1,x_n]\) is called the scope of the model.

\begin{equation}
    \vec{X} = \begin{bmatrix}
        x_1     \\
        x_2     \\
        \vdots  \\
        x_n
    \end{bmatrix}
\end{equation}

Corresponding to each of these input variables \(x_i\), the observation yields a target variable, \(t_i\).
The target vector \(\vec{T}\) is comprised of these observations and maintains the same order as \(\vec{X}\).

\begin{equation}
    \vec{T} = \begin{bmatrix}
        t_1     \\
        t_2     \\
        \vdots  \\
        t_n
    \end{bmatrix}
\end{equation}

The goal is to find a model \(y(x)\) that predicts the target variable \(t\) for a given input variable \(x\).

\begin{equation}
    y(x) = t
\end{equation}

The set of ordered pairs \((x_i, t_i)\) comprises the training data set for this model.
Note that the predicted value \(y_i\) is likely to not be equal to the observation \(t_i\), leading to an error \(e_i\).
The error \(e_i\) should be non-negative so that it can be summed.

\begin{equation}
    y_i = y(x_i)
\end{equation}

\begin{equation}
    e_i = (y_i - t_i)^2
    \label{eqn:error}
\end{equation}

Until now, we have not limited the functional form of the model \(y(x)\) and there are an infinite number of models possible.
We will now consider models that are in the form of a polynomial in \(x\), with no limitations on the order of the polynomial \(m\) or its coefficients \(w_j\).\footnote{Note however, that \(\vec{W}\) is zero-indexed and has \(m+1\) components, while \(\vec{X}\) and \(\vec{T}\) are one-indexed and have \(n\) components.}

\begin{align}
    y(x, \vec{W}) & = w_0 + w_1x + w_2x^2 + ... + w_mx^m \\
                  & = \sum_{j=0}^{j=m}{w_jx^j}
\end{align}

The coefficients of this polynomial, \(w_j\) form the components of the parameters vector \(\vec{W}\) which identifies this model uniquely.
Since this model is linear in its parameters \(\vec{W}\), this is a linear model.

Our goal now is to find the optimal parameters \(\vec{W^*}\) such that the model \(y(x, \vec{W^*})\) best predicts the target variable \(t\) for any input variable \(x\) within the scope of the model.
One way to quantify what constitutes "best prediction" is to use the error function from Equation \ref{eqn:error} and minimize the difference between our prediction and our observation for the training data.

\begin{align}
    E_i        & = \frac{1}{2}(y_i - t_i)^2 = \frac{1}{2}(y(x_i, \vec{W}) - t_i)^2 \\
    E(\vec{W}) & = \frac{1}{2}\sum_{i=1}^{i=n}(y(x_i, \vec{W}) - t_i)^2
\end{align}

The magnitude of this error function is not critical.
We simply need to minimize its value for all possible choices of the model parameters, \(\vec{W}\).
The factor \(\frac{1}{2}\) is inserted for convenience and will disappear when the error is differentiated.

\section{Solution}

In order to find the optimal parameters \(\vec{W^*}\), we need to differentiate the error function and find the zeros.

\begin{align}
    dE(\vec{W}) & = \frac{\partial{E}}{\partial{w_0}}dw_0 + \frac{\partial{E}}{\partial{w_1}}dw_1 + \cdots + \frac{\partial{E}}{\partial{w_m}}dw_m  \\
    dE(\vec{W}) & = \sum_{j=0}^{j=m}{\frac{\partial{E}}{\partial{w_j}}dw_j}                                                                         \\
    dE(\vec{W}) & = 0 \text{ when } \vec{W} = \vec{W^*}                                                                                             \\
    \implies \left.\frac{\partial{E}}{\partial{w_j}}\right|_{w_j=w_j^*} & = 0 \,\forall j \in \{0, 1, \cdots, m\}
\end{align}

Thus, we end up with \(m+1\) linear equations in \(\vec{W}\) that can be solved to find \(\vec{W^*}\).
Each partial derivative expands thusly.

\begin{align}
    \frac{\partial{E}}{\partial{w_j}}                       & = \sum_{i=1}^{i=n}{(y_i - t_i)}\frac{\partial{y_i}}{\partial{w_j}}                \\
                                                            & \frac{\partial{y_i}}{\partial{w_j}} = x_i^j                                       \\
    \text{Substituting, } \frac{\partial{E}}{\partial{w_j}} & = \sum_{i=1}^{i=n}{(y_i - t_i)x_i^j}                                              \\
                                                            & = \sum_{i=1}^{i=n}{(\sum_{k=0}^{k=m}{w_kx_i^k} - t_i)x_i^j}                       \\
                                                            & = \sum_{k=0}^{k=m}{w_k\sum_{i=1}^{i=n}{x_i^k}x_i^j - \sum_{i=1}^{i=n}{t_i}x_i^j}  \\
                                                            & = \sum_{k=0}^{k=m}{w_k\sum_{i=1}^{i=n}{x_i^{j+k}} - \sum_{i=1}^{i=n}{t_i}x_i^j}
\end{align}

Hence, our \(m+1\) linear equations in \(\vec{W^*}\) are as follows.

\begin{equation}
    \sum_{k=0}^{k=m}{w_k^ * \sum_{i=1}^{i=n}{x_i^{j+k}} - \sum_{i=1}^{i=n}{t_i}x_i^j} = 0
\end{equation}

Written out as a matrix,

\begin{equation}
    \begin{bmatrix}
        n                       & \sum_{i=1}^{i=n}{x_i}       & \cdots & \sum_{i=1}^{i=n}{x_i^m}        \\[1em]
        \sum_{i=1}^{i=n}{x_i}   & \sum_{i=1}^{i=n}{x_i^2}     & \cdots & \sum_{i=1}^{i=n}{x_i^{m+1}}    \\[1em]
        \vdots                  & \vdots                      & \ddots & \vdots                         \\[1em]
        \sum_{i=1}^{i=n}{x_i^m} & \sum_{i=1}^{i=n}{x_i^{m+1}} & \cdots & \sum_{i=1}^{i=n}{x_i^{2m}}
    \end{bmatrix}\begin{bmatrix}
        w_0^*   \\[1em]
        w_1^*   \\[1em]
        \vdots  \\[1em]
        w_m^*
    \end{bmatrix} = \begin{bmatrix}
        \sum_{i=1}^{i=n}{t_i}       \\[1em]
        \sum_{i=1}^{i=n}{t_ix_i}    \\[1em]
        \vdots                      \\[1em]
        \sum_{i=1}^{i=n}{t_ix_i^m}
    \end{bmatrix}
\end{equation}

Using \(\sum{}\) for \(\sum_{i=1}^{i=n}{}\) makes this equation look less busy.

\begin{equation}
    \begin{bmatrix}
        n           & \sum{x_i}         & \cdots & \sum{x_i^m}      \\[1em]
        \sum{x_i}   & \sum{x_i^2}       & \cdots & \sum{x_i^{m+1}}  \\[1em]
        \vdots      & \vdots            & \ddots & \vdots           \\[1em]
        \sum{x_i^m} & \sum{x_i^{m+1}}   & \cdots & \sum{x_i^{2m}}
    \end{bmatrix}\begin{bmatrix}
        w_0^*   \\[1em]
        w_1^*   \\[1em]
        \vdots  \\[1em]
        w_m^*
    \end{bmatrix} = \begin{bmatrix}
        \sum{t_i}       \\[1em]
        \sum{t_ix_i}    \\[1em]
        \vdots          \\[1em]
        \sum{t_ix_i^m}
    \end{bmatrix}
\end{equation}

\bibliographystyle{plain}
\bibliography{}

\end{document}

